# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a0spG6hBz1kWYSxpH5rHKwxZVIC5Kgel
"""

!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq
!pip install bitsandbytes einops wandb -Uqqq

import torch
import glob
import pandas as pd
import numpy as np
import re
from peft import get_peft_model, PeftConfig, PeftModel, LoraConfig, prepare_model_for_kbit_training
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, GenerationConfig
from trl import SFTTrainer
from datasets import Dataset

# Path to the JSON file
path = ''
json_file = path + 'dataset_2.json'  # Update this with your actual JSON file name

# Read the JSON file into a DataFrame
df = pd.read_json(json_file)

# Concatenate all 'inputs' fields into a single string
lyrics = '\n'.join(df['inputs'])

# Print the first 200 characters of the concatenated inputs
print(lyrics[:200])

# Cleaning the file by removing/replacing unnecessary characters and removing sections
# that are not lyrics
replace_with_space = ['\u2005', '\u200b', '\u205f', '\xa0', '-']
replace_letters = {'í':'i', 'é':'e', 'ï':'i', 'ó':'o', ';':',', '‘':'\'', '’':'\'', ':':',', 'е':'e'}
remove_list = ['\)', '\(', '–','"','”', '"', '\[.*\]', '.*\|.*', '—']

cleaned_lyrics = lyrics

for old, new in replace_letters.items():
    cleaned_lyrics = cleaned_lyrics.replace(old, new)
for string in remove_list:
    cleaned_lyrics = re.sub(string,'',cleaned_lyrics)
for string in replace_with_space:
    cleaned_lyrics = re.sub(string,' ',cleaned_lyrics)
print(''.join(sorted(set(cleaned_lyrics))))

# Setting aside a portion for training the model and a portion for testing the data to prevent
# the model from overfitting to the data it is tested on
split_point = int(len(cleaned_lyrics)*0.95)
train_data = cleaned_lyrics[:split_point]
test_data = cleaned_lyrics[split_point:]
train_data_seg = []
for i in range(0, len(train_data), 500):
        text = train_data[i:min(i+500, len(train_data))]
        train_data_seg.append(text)
train_data_seg = Dataset.from_dict({'text':train_data_seg})
print(len(train_data_seg))

# You will need to create a Hugging Face account if you do not have one,
# and then generate a write token to enter in the widget below
from huggingface_hub import notebook_login
notebook_login()

# Loading the model with double quantization
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v0.1"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

# Creating tokenizer and defining the pad token
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

def generate_data(query, model, tokenizer):
    inputs = tokenizer(query, return_tensors="pt")
    generation_config = {
       "max_length": 250,  # Increase the maximum length of the generated sequence
        "pad_token_id": tokenizer.eos_token_id,
        "repetition_penalty": 1.2,  # Adjust repetition penalty
        "temperature": 0.7,  # Control randomness in the generation
        "top_p": 0.9,  # Use nucleus sampling
        "eos_token_id": tokenizer.eos_token_id
    }
    outputs = model.generate(input_ids=inputs.input_ids.to(model.device), **generation_config)
    text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return text_output[len(query):].strip()

# Setting arguments for low-rank adaptation

model = prepare_model_for_kbit_training(model)

lora_alpha = 32 # The weight matrix is scaled by lora_alpha/lora_rank, so I set lora_alpha = lora_rank to remove scaling
lora_dropout = 0.05
lora_rank = 32

peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_rank,
    bias="none",  # setting to 'none' for only training weight params instead of biases
    task_type="CAUSAL_LM")

peft_model = get_peft_model(model, peft_config)

# Setting training arguments

output_dir = "mukulb/tinyllama-strisakhi" # Model repo on your hugging face account where you want to save your model
per_device_train_batch_size = 3
gradient_accumulation_steps = 2
optim = "paged_adamw_32bit"
save_strategy="steps"
save_steps = 10
logging_steps = 10
learning_rate = 2e-3
max_grad_norm = 0.3 # Sets limit for gradient clipping
max_steps = 200     # Number of training steps
warmup_ratio = 0.03 # Portion of steps used for learning_rate to warmup from 0
lr_scheduler_type = "cosine" # I chose cosine to avoid learning plateaus

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    lr_scheduler_type=lr_scheduler_type,
    push_to_hub=False,
    report_to='none'
)

trainer = SFTTrainer(
    model=peft_model,
    train_dataset=train_data_seg,
    peft_config=peft_config,
    max_seq_length=500,
    dataset_text_field='text',
    tokenizer=tokenizer,
    args=training_arguments
)
peft_model.config.use_cache = False

trainer.train()

# Save the model after training
model.save_pretrained(output_dir)

print(f"Model saved successfully to {output_dir}")

model_finetune="mukulb/tinyllama-strisakhi"
model_load = AutoModelForCausalLM.from_pretrained(model_finetune)

# Chat loop
print("Welcome to the chat with the model! Type 'exit' to end the conversation.")
while True:
    user_input = input("You: ")
    if user_input.lower() == 'exit':
        print("Chat ended.")
        break
    response = generate_data(user_input, model, tokenizer)
    print("Model:", response)